{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - Programação Dinâmica.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN62MdjXB8hlKgc+PlVI8wd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aplneto/IF998/blob/main/04_-_Programa%C3%A7%C3%A3o_Din%C3%A2mica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programação Dinâmica"
      ],
      "metadata": {
        "id": "d3QYmZ10UtS-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yBVz-6Vw1tJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efbcce3f-154a-4d4d-8b15-73cf971936cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "fibonacci = lambda x: 1 if (x <= 1) else fibonacci(x-1)+fibonacci(x-2)\n",
        "\n",
        "print(fibonacci(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fib_mem(x):\n",
        "  f = [1, 1]\n",
        "  for i in range(2, x+1):\n",
        "    f.append(f[i-1]+f[i-2])\n",
        "  return f[x]\n",
        "\n",
        "print(fib_mem(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w8H2TOtWGMJ",
        "outputId": "bc42bc32-489e-4149-ae34-20d747496f78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No contexto de MDP\n",
        "\n",
        "* O processo de decisão de Markov satisfaz as condições:\n",
        "  * A equação de Bellman fornece uma decomposição recursiva:<br>\n",
        "  $\n",
        "  v_{k+1}(s)\\doteq\\mathbb{E}\\left[R_{t+1}+\\gamma v_k(S_{t+1})\\middle|S_t=s\\right]\n",
        "  $\n",
        "  * A função de valor armazena soluções para reutilização (memória cache)"
      ],
      "metadata": {
        "id": "OZEBfOo4Y-_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Evaluation\n",
        "* Problema: Avaliar uma politica $\\pi(a|s)$\n",
        "* Solução: Utilizar a equação de Bellman\n",
        "  * Evolução: $v_1\\to v_2\\to ... \\to v_k$\n",
        "  * Para cada iteração $k+1$\n",
        "  * Para todo $s \\in S$\n",
        "  * Atualiza $v_{k+1}(s)$ a partir de $v_k(s\\prime)$<br>\n",
        "  $s\\prime$ sendo o sucessor de $s$ de acordo com a política $\\pi$"
      ],
      "metadata": {
        "id": "oy-Nn1sBaJqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Avaliar $v_\\pi(s)$ para a política aleatória neste ambiente:\n",
        "\n",
        "\\begin{array}{|c|c|c|c|}\\hline\\\n",
        "&1&2&3\\\\\\hline\n",
        "4&5&6&7\\\\\\hline\n",
        "8&9&10&11\\\\\\hline\n",
        "12&13&14&\\hline\\\\\\hline\n",
        "\\end{array}\n",
        "\n",
        "$\\pi(a|s)=0.25\\space\\left(\\uparrow\\downarrow\\leftarrow\\rightarrow\\right)$<br>\n",
        "$R-t=-1\\space\\text{em todas as transições.}$<br>\n",
        "$R_t=0\\space\\text{em ambos os estados terminais, impedindo o agente de se mover}$"
      ],
      "metadata": {
        "id": "ve71SrknbmQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "pi = numpy.asarray([0.25] * 4)\n",
        "\n",
        "# Recompensas\n",
        "r = -1 * numpy.ones((4, 4))\n",
        "# Estados terminais\n",
        "r[0, 0] = 0\n",
        "r[-1, -1] = 0"
      ],
      "metadata": {
        "id": "4qXBHbXQbkol"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mover(pos, d):\n",
        "  '''\n",
        "  Retorna a nova posição do agente a partir de sua posição anterior e da direção\n",
        "  do movimento\n",
        "    Arguments:\n",
        "      pos (int, int): posição atual do agente\n",
        "      d (str or int): direção do movimento seguindo as regras\n",
        "        - n ou 0: Norte \\u2191\n",
        "        - s ou 1: Sul \\u2193\n",
        "        - l ou 2: Leste \\u2192\n",
        "        - o ou 3: Oeste \\u2190\n",
        "    Returns:\n",
        "      nova_pos (int, int): nova posição do agente\n",
        "  '''\n",
        "  global r\n",
        "  if r[pos] == 0:\n",
        "    return pos\n",
        "  if isinstance(d, str):\n",
        "    d = 'nslo'.index(d.lower())\n",
        "  funcao_movimento = (\n",
        "      lambda row, column: (row-1, column), # Norte\n",
        "      lambda row, column: (row+1, column), # Sul\n",
        "      lambda row, column: (row, column+1), # Leste\n",
        "      lambda row, column: (row, column-1) # Oeste\n",
        "  )[d]\n",
        "  valida = (\n",
        "      lambda pos: not((pos[0] < 0 or pos[0] > 3) or (pos[1] < 0 or pos[1] > 3))\n",
        "  )\n",
        "  nova_pos = funcao_movimento(*pos)\n",
        "  return nova_pos if valida(nova_pos) else pos"
      ],
      "metadata": {
        "id": "PaFeac9QiJNR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\n",
        "v_{k+1}(s) = \\mathbb{E}_\\pi[R_{t+1}+\\gamma v_k(S_{t+1}) | St = s]\\\\\n",
        "\\quad\\qquad= \\sum_a\\pi(a|s)\\sum_{s\\prime,r}p(s\\prime,r|s,a)\\left[r+\\gamma v_k(s\\prime)\\right],\\quad (4.5)\n",
        "$"
      ],
      "metadata": {
        "id": "j_LavvUnobiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = numpy.zeros((4, 4))\n",
        "print(\"Passo 0: \")\n",
        "print(numpy.round(v, 1))\n",
        "gamma = 0.9\n",
        "for k in range(10):\n",
        "  vback = v.copy() # Tabela de backup\n",
        "  for row in range(4):\n",
        "    for column in range(4):\n",
        "      values = vback[tuple(zip(*[mover((row, column), d) for d in 'nslo']))]\n",
        "      v[row][column] = numpy.sum(\n",
        "          pi * (r[row][column] + (gamma * values))\n",
        "      )\n",
        "  if (k in [0, 1, 2, 9]):\n",
        "    print(\"Passo %i\" % (k+1))\n",
        "    print(numpy.round(v, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm0Ex7_JlR7U",
        "outputId": "d9b2f304-0288-4787-bd63-59d934a41426"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passo 0: \n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Passo 1\n",
            "[[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n",
            "Passo 2\n",
            "[[ 0.  -1.7 -1.9 -1.9]\n",
            " [-1.7 -1.9 -1.9 -1.9]\n",
            " [-1.9 -1.9 -1.9 -1.7]\n",
            " [-1.9 -1.9 -1.7  0. ]]\n",
            "Passo 3\n",
            "[[ 0.  -2.2 -2.7 -2.7]\n",
            " [-2.2 -2.6 -2.7 -2.7]\n",
            " [-2.7 -2.7 -2.6 -2.2]\n",
            " [-2.7 -2.7 -2.2  0. ]]\n",
            "Passo 10\n",
            "[[ 0.  -4.3 -5.7 -6. ]\n",
            " [-4.3 -5.3 -5.7 -5.7]\n",
            " [-5.7 -5.7 -5.3 -4.3]\n",
            " [-6.  -5.7 -4.3  0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atualização síncrona:\n",
        "* Passa por todos os estados\n",
        "* Atualiza toda a tabela \"simultaneamente\"\n",
        "* Não é _inplace_. Requer uma tabela de **backup**"
      ],
      "metadata": {
        "id": "uyewr7Rcso3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Policy Evaluation, for estimating $V \\approx v_\\pi$\n",
        "\n",
        "Input $\\pi$, a política a ser avaliada\n",
        "\n",
        "Parâmetros do algoritmo: um pequeno limiar $\\theta > 0$ determina a acurácia da estimativa (o mínimo necessário para a mudança da função de valor a cada passo)\n",
        "\n",
        "Initializar $V(s)$, para todo $s\\in \\mathscr{S}^+$, abritrariamente, exceto para $V(\\text{terminal}) = 0$\n",
        "\n",
        "$\n",
        "\\text{Loop:}\\\\\n",
        "\\quad\\Delta\\gets0\\\\\n",
        "\\quad\\text{Loop para todo } s\\in \\mathscr{S}:\\\\\n",
        "\\quad\\quad v\\gets V(s)\\\\\n",
        "\\quad\\quad V(s)\\gets\\sum_a\\pi(a|s)\\sum_{s\\prime,r}p(s\\prime,r|s,a)[r+\\gamma V(s\\prime)]\\\\\n",
        "\\quad\\quad\\Delta\\gets\\max(\\Delta,|v-V(s)|)\\\\\n",
        "\\text{até}\\space\\Delta<\\theta\n",
        "$"
      ],
      "metadata": {
        "id": "0IMKHo25tNem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Observe o que ocorre com a política gulosa:<br>\n",
        "$\\pi(s)\\gets \\text{argmax}_a\\sum_{s\\prime,r}p(s\\prime,r|s,a)[r+\\gamma V(s\\prime)]$"
      ],
      "metadata": {
        "id": "61AvPLb_yK5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mostrar_politica(pi: numpy.ndarray):\n",
        "  '''\n",
        "  Função auxiliar para exibir a matriz de políticas com setas ao invés de\n",
        "  números\n",
        "    Arguments:\n",
        "      pi ([4, 4, 4]): Política de ações a ser exibida\n",
        "    Returns:\n",
        "      friendly_pi: Matriz de política user-friendly\n",
        "  '''\n",
        "  # Converte o binário do vetor para inteiro\n",
        "  binary = lambda arr: numpy.sum(1*(2**numpy.where(arr == 1)[0]))\n",
        "  friendly_pi = []\n",
        "  setas = [\n",
        "      '', '\\u2191', '\\u2193', '\\u2195', '\\u2192', '\\u2197', '\\u2198', 'nsl',\n",
        "      '\\u2190', '\\u2196', '\\u2199', 'nso', '\\u2194', 'nlo', 'slo', '\\u271a'\n",
        "  ]\n",
        "  for row in range(4):\n",
        "    friendly_pi.append([])\n",
        "    for column in range(4):\n",
        "      i = binary(pi[row][column])\n",
        "      friendly_pi[row].append(setas[i])\n",
        "  return friendly_pi"
      ],
      "metadata": {
        "id": "OdGiE1PS_UfR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_greedy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQbfF0l6n30",
        "outputId": "9462ac4c-f510-4fac-9452-4cb72fa5ebcf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [0., 1., 0., 1.]],\n",
              "\n",
              "       [[1., 0., 0., 0.],\n",
              "        [1., 0., 0., 1.],\n",
              "        [0., 1., 0., 1.],\n",
              "        [0., 1., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 0., 0.],\n",
              "        [1., 0., 1., 0.],\n",
              "        [0., 1., 1., 0.],\n",
              "        [0., 1., 0., 0.]],\n",
              "\n",
              "       [[1., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_greedy = numpy.zeros((4, 4, 4))\n",
        "v = numpy.zeros((4, 4))\n",
        "\n",
        "for k in range(11):\n",
        "  vback = v.copy()\n",
        "  for row in range(4):\n",
        "    for column in range(4):\n",
        "      directions = [mover((row, column), d) for d in 'nslo']\n",
        "      values = vback[tuple(zip(*directions))]\n",
        "      # valor da política aleatória\n",
        "      v[row][column] = numpy.sum(\n",
        "          pi * (r[row][column] + (gamma * values))\n",
        "      )\n",
        "      action_value = (r[row][column] + (gamma * values))\n",
        "      # lista as ações que maximizam a política gulosa no atual estado\n",
        "      greed_actions = numpy.argwhere(action_value == numpy.amax(action_value))\n",
        "      # código auxiliar para listar as funções como representações binárias\n",
        "      aux = numpy.zeros((greed_actions.size, 4))\n",
        "      aux[numpy.arange(greed_actions.size),greed_actions] = 1\n",
        "      pi_greedy[row][column] = aux[0]\n",
        "      for i in range(1, len(aux)):\n",
        "        pi_greedy[row][column] = (\n",
        "            numpy.logical_or(pi_greedy[row][column], aux[i])\n",
        "        )\n",
        "  if k in [0, 1, 2, 3, 10]:\n",
        "    print(\"Passo %i\" % k)\n",
        "    politica_greedy = mostrar_politica(pi_greedy)\n",
        "    for x in range(4):\n",
        "      print(numpy.round(v[x], 1), politica_greedy[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DF30yKqydWH",
        "outputId": "04323a1c-23da-4a71-c22d-34c909934508"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passo 0\n",
            "[ 0. -1. -1. -1.] ['✚', '✚', '✚', '✚']\n",
            "[-1. -1. -1. -1.] ['✚', '✚', '✚', '✚']\n",
            "[-1. -1. -1. -1.] ['✚', '✚', '✚', '✚']\n",
            "[-1. -1. -1.  0.] ['✚', '✚', '✚', '✚']\n",
            "Passo 1\n",
            "[ 0.  -1.7 -1.9 -1.9] ['✚', '←', '✚', '✚']\n",
            "[-1.7 -1.9 -1.9 -1.9] ['↑', '✚', '✚', '✚']\n",
            "[-1.9 -1.9 -1.9 -1.7] ['✚', '✚', '✚', '↓']\n",
            "[-1.9 -1.9 -1.7  0. ] ['✚', '✚', '→', '✚']\n",
            "Passo 2\n",
            "[ 0.  -2.2 -2.7 -2.7] ['✚', '←', '←', '✚']\n",
            "[-2.2 -2.6 -2.7 -2.7] ['↑', '↖', '✚', '↓']\n",
            "[-2.7 -2.7 -2.6 -2.2] ['↑', '✚', '↘', '↓']\n",
            "[-2.7 -2.7 -2.2  0. ] ['✚', '→', '→', '✚']\n",
            "Passo 3\n",
            "[ 0.  -2.7 -3.3 -3.4] ['✚', '←', '←', '↙']\n",
            "[-2.7 -3.2 -3.4 -3.3] ['↑', '↑', '↙', '↓']\n",
            "[-3.3 -3.4 -3.2 -2.7] ['↑', '↗', '↘', '↓']\n",
            "[-3.4 -3.3 -2.7  0. ] ['↗', '→', '→', '✚']\n",
            "Passo 10\n",
            "[ 0.  -4.4 -5.9 -6.3] ['✚', '←', '←', '↙']\n",
            "[-4.4 -5.5 -5.9 -5.9] ['↑', '↖', '↙', '↓']\n",
            "[-5.9 -5.9 -5.5 -4.4] ['↑', '↗', '↘', '↓']\n",
            "[-6.3 -5.9 -4.4  0. ] ['↗', '→', '→', '✚']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration\n",
        "\n",
        "* Encontrar a política ótima através de:\n",
        "  * Policy Evalutaion\n",
        "  * Policy Improvement $\\pi\\prime(s)\\doteq\\text{argmax}_aq_\\pi(s,a)$<br><br>\n",
        "  $\n",
        "  \\pi_0\\xrightarrow[]{E}v_{\\pi_0}\\xrightarrow[]{I}\\pi_1\\xrightarrow[]{E}v_{\\pi_1}\\xrightarrow[]{I}\\pi_2\\xrightarrow[]{E}...\\xrightarrow[]{I}\\pi_*\\xrightarrow[]{E}v_*\n",
        "  $\n",
        "* Se repete por K passos\n",
        "* Sempre converge para a solução ótima\n"
      ],
      "metadata": {
        "id": "kLL6gHxnY2Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Iteration (usindo policy evaluation iterativa) para estimativa de $\\pi\\approx\\pi_*$\n",
        "\n",
        "1. Inicialização<br>\n",
        "$\n",
        "V(s)\\in\\mathbb{N}\\space\\text{e}\\pi(s)\\in\\mathcal{A}(s)\\space\\text{definidos arbitráriamente para todo}s\\in\\mathcal{S}\n",
        "$\n",
        "1. Policy Evaluation:<br>\n",
        "$\n",
        "\\text{Loop:}\\\\\n",
        "\\quad\\Delta\\gets0\\\\\n",
        "\\quad\\text{Loop para cada}s\\in\\mathcal{S}:\\\\\n",
        "\\qquad v\\gets V(s)\\\\\n",
        "\\qquad V(s)\\gets\\sum_{s\\prime,r}p(s\\prime,r|s,\\pi(s))[r+\\gamma V(s\\prime)]\\\\\n",
        "\\qquad\\Delta\\gets\\max(\\Delta,|v-V(s)|)\\\\\n",
        "\\quad\\text{até }\\Delta<\\theta\\text{ um número real positivo e pequano que determina a acurácia da estimativa}\n",
        "$\n",
        "1. Policy Improvement<br>\n",
        "$\n",
        "\\text{policy_stable}\\gets\\text{true}\\\\\n",
        "\\text{Para cada}s\\in\\mathcal{S}:\\\\\n",
        "\\quad\\text{old_action}\\gets\\pi(s)\\\\\n",
        "\\quad\\pi(s)\\gets \\text{argmax}_a\\sum_{s\\prime,r}p(s\\prime,r|s,a)[r+\\gamma V(s\\prime)]\\\\\n",
        "\\quad\\text{If old_action}\\ne\\pi(s)\\text{, then policy_stable}\\gets\\text{false}\n",
        "\\text{If policy_stable, then stop and return}V\\approx v_*\\text{; else go to 2.}\n",
        "$"
      ],
      "metadata": {
        "id": "TiAXjLC0LrB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V = numpy.zeros((4,4)) # Função valor\n",
        "politica = numpy.random.randint(0, 3, (4, 4)) # Política aleatória\n",
        "gamma = 0.9\n",
        "theta = 1e-20\n",
        "\n",
        "policy_stable = False\n",
        "while not policy_stable:\n",
        "  while True:\n",
        "    delta = 0\n",
        "    v = V.copy()\n",
        "    for i in range(4):\n",
        "      for j in range(4):\n",
        "        # Estado futuro de acordo com a política\n",
        "        future_state = mover((i,j), politica[(i,j)])\n",
        "        # Atualização da função valor\n",
        "        V[i][j] = 1 * (r[(i,j)] + (gamma * v[future_state]))\n",
        "        delta = max(delta, abs(v[(i,j)] - V[(i,j)]))\n",
        "    if delta < theta:\n",
        "      break\n",
        "\n",
        "  while not policy_stable:\n",
        "    policy_stable = True\n",
        "    for i in range(4):\n",
        "      for j in range(4):\n",
        "        old_action = politica[(i,j)]\n",
        "        future_states = [mover((i, j), d) for d in 'nslo']\n",
        "        # Valor dos estados futuros possíveis\n",
        "        state_values = V[tuple(zip(*future_states))]\n",
        "        action_values = (r[i][j] + (gamma * state_values))\n",
        "        # Função valor\n",
        "        politica[(i,j)] = numpy.argmax(action_values)\n",
        "        if old_action != politica[(i,j)]:\n",
        "          policy_stable = False\n",
        "\n",
        "print(politica)\n",
        "print(V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y1ZR8sJlaCd",
        "outputId": "5eff7e3f-0532-4b91-af2a-1c748c07bc9c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 3 0 0]\n",
            " [0 0 0 1]\n",
            " [0 0 2 1]\n",
            " [0 0 2 0]]\n",
            "[[  0. -10. -10. -10.]\n",
            " [-10. -10. -10. -10.]\n",
            " [-10. -10. -10.  -1.]\n",
            " [-10. -10. -10.   0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration\n",
        "\n",
        "* Policy Iteration pode se tornar lento e custoso\n",
        "  * Obs.: não é necessário convergir totalmente a função de valor para se obter melhorias significativas na política\n",
        "* Value iteration: melhora a política após um único passo de Policy Evaluation\n",
        "* Atualização corresponde a equação de otimização de Bellman\n",
        "* Bellman Iteration: <br>\n",
        "$\n",
        "v_{k+1}(s) \\doteq \\mathbb{E}_\\pi[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s]\\\\\n",
        "\\qquad=\\sum_a\\pi(a|s)\\sum_{s\\prime,r}p(s\\prime,r|s,a)[r+\\gamma v_k(s\\prime)]\n",
        "$\n",
        "$\n",
        "v_{k+1}(s)=\\max_a\\mathbb{E}[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s,A_t=a]\\\\\n",
        "\\qquad=\\max_a\\sum_{s\\prime,r}p(s\\prime,r|s,a)(r+\\gamma v_k(s\\prime))\n",
        "$"
      ],
      "metadata": {
        "id": "FMEkbEuG3L3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration para estimativa de $\\pi\\approx\\pi_*$\n",
        "\n",
        "Parâmetros do algoritmo: um pequeno limiar $\\theta > 0$ que determina a acurácia da estimativa\n",
        "\n",
        "Inicializar $V(s)$, para todo $s\\in\\mathcal{S}^+$, arbitrariamente exceto em $V(\\text{terminal}) = 0$.<br>\n",
        "$\n",
        "\\text{Loop:}\\\\\n",
        "\\quad\\Delta\\gets0\\\\\n",
        "\\quad\\text{Loop para cada}\\space s\\in\\mathcal{S}:\\\\\n",
        "\\qquad v\\gets V(s)\\\\\n",
        "\\qquad V(s)\\gets\\max_a\\sum_{s\\prime,r}p(s\\prime,r|s,a)[r+\\gamma V(s\\prime)]\\\\\n",
        "\\qquad\\Delta\\gets\\max(\\Delta,|v-V(s)|)\\\\\n",
        "\\text{até que }\\Delta<0\n",
        "$\n",
        "\n",
        "Retorna uma polítia determinística, $\\pi\\approx\\pi_*$, tal que:<br>\n",
        "$\n",
        "\\pi(s)=\\text{argmax}_a\\sum_{s\\prime,r}p(s\\prime,r|s,a)[r+\\gamma V(s\\prime)]\n",
        "$"
      ],
      "metadata": {
        "id": "Cf9YKZkWAxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politica = numpy.random.randint(0,3,(4,4))\n",
        "V = numpy.zeros((4, 4))\n",
        "\n",
        "policy_stable = False\n",
        "theta = 0.1\n",
        "while True:\n",
        "  delta = 0\n",
        "  v = V.copy()\n",
        "  for i in range(4):\n",
        "    for j in range(4):\n",
        "      directions = [mover((i, j), d) for d in 'nslo']\n",
        "      # Valor dos estados futuros possíveis\n",
        "      state_values = v[tuple(zip(*directions))]\n",
        "      # Valor de cada ação possível\n",
        "      action_value = (r[i][j] + (gamma * state_values))\n",
        "      # Função valor\n",
        "      V[i][j] = 1 * numpy.max(action_value)\n",
        "      # atualização da política com a ação gulosa\n",
        "      politica[i][j] = numpy.argmax(action_value)\n",
        "      delta = max(delta, abs(v[i][j] - V[i][j]))\n",
        "  if delta < theta:\n",
        "    break\n",
        "\n",
        "print(V)\n",
        "print(politica)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yJ6Hs7n3fgw",
        "outputId": "d1da6073-8d2c-45fd-c1be-e36246cab176"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.   -1.   -1.9  -2.71]\n",
            " [-1.   -1.9  -2.71 -1.9 ]\n",
            " [-1.9  -2.71 -1.9  -1.  ]\n",
            " [-2.71 -1.9  -1.    0.  ]]\n",
            "[[0 3 3 1]\n",
            " [0 0 0 1]\n",
            " [0 0 1 1]\n",
            " [0 2 2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "politica"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkkjf6pf74Sz",
        "outputId": "a0fd8621-ef9f-4ff8-d29f-4bf862500d3c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 3, 3, 1],\n",
              "       [0, 0, 0, 1],\n",
              "       [0, 0, 1, 1],\n",
              "       [0, 2, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PD Assíncrona\n",
        "\n",
        "* Até então, supomos que a atualização da função valor ocorreria de forma síncrona, atalizando todos os valores simultaneamente.\n",
        "* Muitas vezes isso é proibitivo devido ao grande número de estados.\n",
        "* Porém, de fato isto não é necessário, é possível atualizar apenas alguns dos estados, a medida que se tem informações de novas transições.\n",
        "* A convergência também pode ser demonstrada nestas situações.\n",
        "\n",
        "<!-- resumindo: Ao invés de criar uma cópia da tabela e atualizar todos os valores numa tabela separada, é possível fazer a atualização *in-place* e ainda convergir para a solução do problema -->"
      ],
      "metadata": {
        "id": "RLhCF78XEV8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eficiência de PD\n",
        "\n",
        "* Tempo polinomial em |S| e |A|\n",
        "* A dimensionalidade do problema aumenta exponencialmente com a quantidade de estados e variáveis de estado\n",
        "* PD assíncrona é mais utilizada em problemas com muitos estados\n"
      ],
      "metadata": {
        "id": "gGVbIVQpE-ex"
      }
    }
  ]
}